"""
CLIP Image Similarity Service
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç CLIP –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–∏ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∑–¥–∞–Ω–∏–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
"""

import os
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from PIL import Image
import torch
import open_clip
from pathlib import Path
import pickle

logger = logging.getLogger(__name__)

class CLIPSimilarityService:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CLIP –º–æ–¥–µ–ª–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.preprocess = None
        self.tokenizer = None
        self.index = None
        self.image_paths = []
        self.image_metadata = []
        
        # –ü—É—Ç—å –∫ –∫—ç—à—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.cache_dir = Path("data/clip_cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.embeddings_file = self.cache_dir / "embeddings.pkl"
        
        self._initialize_model()
        self._load_or_create_index()
        
    def _initialize_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ CLIP –º–æ–¥–µ–ª–∏"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º ViT-B/32 - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
            model_name = 'ViT-B-32'
            pretrained = 'openai'
            
            logger.info(f"ü§ñ Loading CLIP model: {model_name}")
            self.model, _, self.preprocess = open_clip.create_model_and_transforms(
                model_name, 
                pretrained=pretrained,
                device=self.device
            )
            self.tokenizer = open_clip.get_tokenizer(model_name)
            
            self.model.eval()  # –†–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            
            logger.info(f"‚úÖ CLIP model loaded on {self.device}")
            
        except Exception as e:
            logger.error(f"Failed to load CLIP model: {e}")
            raise
    
    def _load_or_create_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ"""
        try:
            if self.embeddings_file.exists():
                logger.info("üìÇ Loading existing CLIP embeddings...")
                with open(self.embeddings_file, 'rb') as f:
                    data = pickle.load(f)
                    self.image_paths = data['paths']
                    self.image_metadata = data['metadata']
                    embeddings = data['embeddings']
                
                # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
                import faiss
                dimension = embeddings.shape[1]
                self.index = faiss.IndexFlatL2(dimension)  # L2 distance
                self.index.add(embeddings)
                
                logger.info(f"‚úÖ Loaded {len(self.image_paths)} image embeddings")
            else:
                logger.info("üìù No existing index found, will create on first use")
                import faiss
                self.index = faiss.IndexFlatL2(512)  # CLIP ViT-B/32 dimension
                
        except Exception as e:
            logger.error(f"Error loading index: {e}")
            import faiss
            self.index = faiss.IndexFlatL2(512)
    
    def encode_image(self, image_path: str) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            –í–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (512-–º–µ—Ä–Ω—ã–π –¥–ª—è ViT-B/32)
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(image_path).convert('RGB')
            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            with torch.no_grad():
                image_features = self.model.encode_image(image_tensor)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            
            return image_features.cpu().numpy()[0]
            
        except Exception as e:
            logger.error(f"Error encoding image {image_path}: {e}")
            return None
    
    def encode_text(self, text: str) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
        –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é: "–∫—Ä–∞—Å–Ω–æ–µ –∫–∏—Ä–ø–∏—á–Ω–æ–µ –∑–¥–∞–Ω–∏–µ —Å –∫–æ–ª–æ–Ω–Ω–∞–º–∏"
        
        Args:
            text: –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            
        Returns:
            –í–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        """
        try:
            text_tokens = self.tokenizer([text]).to(self.device)
            
            with torch.no_grad():
                text_features = self.model.encode_text(text_tokens)
                text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
            return text_features.cpu().numpy()[0]
            
        except Exception as e:
            logger.error(f"Error encoding text: {e}")
            return None
    
    def find_similar_images(self, 
                           image_path: str, 
                           top_k: int = 5,
                           similarity_threshold: float = 0.75) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∏–Ω–¥–µ–∫—Å–µ
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –¥–ª—è –ø–æ–∏—Å–∫–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            similarity_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if self.index.ntotal == 0:
            logger.warning("Index is empty, no similar images to find")
            return []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.encode_image(image_path)
            if query_embedding is None:
                return []
            
            # –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ
            query_embedding = query_embedding.reshape(1, -1).astype('float32')
            distances, indices = self.index.search(query_embedding, min(top_k, self.index.ntotal))
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º L2 distance –≤ cosine similarity
            # similarity = 1 - (distance / 2)  –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
            similarities = 1 - (distances[0] / 2)
            
            results = []
            for idx, similarity in zip(indices[0], similarities):
                if similarity >= similarity_threshold:
                    result = {
                        'image_path': self.image_paths[idx],
                        'similarity': float(similarity),
                        'metadata': self.image_metadata[idx] if idx < len(self.image_metadata) else {}
                    }
                    results.append(result)
            
            logger.info(f"üîç Found {len(results)} similar images (threshold: {similarity_threshold})")
            return results
            
        except Exception as e:
            logger.error(f"Error finding similar images: {e}")
            return []
    
    def add_image_to_index(self, 
                          image_path: str, 
                          metadata: Dict[str, Any] = None) -> bool:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∏–Ω–¥–µ–∫—Å
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, –∞–¥—Ä–µ—Å, –∏ —Ç.–¥.)
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ
        """
        try:
            embedding = self.encode_image(image_path)
            if embedding is None:
                return False
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å
            embedding = embedding.reshape(1, -1).astype('float32')
            self.index.add(embedding)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.image_paths.append(image_path)
            self.image_metadata.append(metadata or {})
            
            logger.debug(f"Added image to index: {image_path}")
            return True
            
        except Exception as e:
            logger.error(f"Error adding image to index: {e}")
            return False
    
    def build_index_from_database(self, photos_with_coords: List[Dict[str, Any]]) -> int:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            photos_with_coords: –°–ø–∏—Å–æ–∫ —Ñ–æ—Ç–æ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ –∏–∑ –ë–î
            
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        logger.info(f"üèóÔ∏è Building CLIP index from {len(photos_with_coords)} photos...")
        
        added_count = 0
        embeddings_list = []
        
        for photo in photos_with_coords:
            image_path = photo.get('file_path')
            if not image_path or not os.path.exists(image_path):
                continue
            
            embedding = self.encode_image(image_path)
            if embedding is not None:
                embeddings_list.append(embedding)
                self.image_paths.append(image_path)
                self.image_metadata.append({
                    'lat': photo.get('lat'),
                    'lon': photo.get('lon'),
                    'address': photo.get('address_data'),
                    'photo_id': photo.get('id')
                })
                added_count += 1
                
                if added_count % 100 == 0:
                    logger.info(f"Processed {added_count} images...")
        
        if embeddings_list:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
            embeddings_array = np.array(embeddings_list).astype('float32')
            
            import faiss
            dimension = embeddings_array.shape[1]
            self.index = faiss.IndexFlatL2(dimension)
            self.index.add(embeddings_array)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self._save_index(embeddings_array)
            
            logger.info(f"‚úÖ Built index with {added_count} images")
        
        return added_count
    
    def _save_index(self, embeddings: np.ndarray):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ —Ñ–∞–π–ª"""
        try:
            data = {
                'paths': self.image_paths,
                'metadata': self.image_metadata,
                'embeddings': embeddings
            }
            
            with open(self.embeddings_file, 'wb') as f:
                pickle.dump(data, f)
            
            logger.info(f"üíæ Saved index to {self.embeddings_file}")
            
        except Exception as e:
            logger.error(f"Error saving index: {e}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏–Ω–¥–µ–∫—Å–∞"""
        return {
            'total_images': self.index.ntotal if self.index else 0,
            'dimension': 512,
            'model': 'ViT-B/32',
            'device': self.device,
            'cache_exists': self.embeddings_file.exists()
        }
