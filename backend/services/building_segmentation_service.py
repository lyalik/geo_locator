"""
Building Segmentation Service
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç semantic segmentation –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∫–æ–Ω—Ç—É—Ä–æ–≤ –∑–¥–∞–Ω–∏–π
–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–º–∏ —Å–Ω–∏–º–∫–∞–º–∏
"""

import os
import logging
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from PIL import Image
import torch
import cv2
from pathlib import Path

logger = logging.getLogger(__name__)

class BuildingSegmentationService:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.input_size = (512, 512)
        
        self._initialize_model()
        
    def _initialize_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        try:
            import segmentation_models_pytorch as smp
            
            logger.info("üèóÔ∏è Loading DeepLabV3+ segmentation model...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º DeepLabV3+ —Å ResNet50 backbone
            # –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∞ –Ω–∞ Cityscapes (–≥–æ—Ä–æ–¥—Å–∫–∏–µ —Å—Ü–µ–Ω—ã)
            self.model = smp.DeepLabV3Plus(
                encoder_name="resnet50",
                encoder_weights="imagenet",
                classes=1,  # Binary segmentation: building vs background
                activation='sigmoid'
            )
            
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"‚úÖ Segmentation model loaded on {self.device}")
            
        except ImportError:
            logger.warning("‚ö†Ô∏è segmentation-models-pytorch not installed")
            self.model = None
        except Exception as e:
            logger.error(f"Failed to load segmentation model: {e}")
            self.model = None
    
    def segment_buildings(self, image_path: str) -> Optional[Dict[str, Any]]:
        """
        –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç—É—Ä–æ–≤ –∑–¥–∞–Ω–∏–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å –∫–æ–Ω—Ç—É—Ä–∞–º–∏ –∏ –º–∞—Å–∫–∞–º–∏
        """
        if not self.model:
            return None
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(image_path).convert('RGB')
            original_size = image.size
            
            # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
            image_resized = image.resize(self.input_size)
            image_array = np.array(image_resized).astype(np.float32) / 255.0
            image_tensor = torch.from_numpy(image_array).permute(2, 0, 1).unsqueeze(0)
            image_tensor = image_tensor.to(self.device)
            
            # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
            with torch.no_grad():
                mask = self.model(image_tensor)
                mask = mask.squeeze().cpu().numpy()
            
            # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –º–∞—Å–∫–∏
            binary_mask = (mask > 0.5).astype(np.uint8)
            
            # Resize –æ–±—Ä–∞—Ç–Ω–æ –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
            binary_mask_resized = cv2.resize(
                binary_mask, 
                original_size, 
                interpolation=cv2.INTER_NEAREST
            )
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç—É—Ä–æ–≤
            contours = self._extract_contours(binary_mask_resized)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            features = self._compute_features(binary_mask_resized, contours)
            
            result = {
                'success': True,
                'mask': binary_mask_resized,
                'contours': contours,
                'features': features,
                'building_area_ratio': features['building_ratio'],
                'num_buildings': len(contours)
            }
            
            logger.info(f"üèóÔ∏è Segmentation: found {len(contours)} buildings, area ratio: {features['building_ratio']:.2%}")
            
            return result
            
        except Exception as e:
            logger.error(f"Error in building segmentation: {e}")
            return None
    
    def _extract_contours(self, binary_mask: np.ndarray) -> List[np.ndarray]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç—É—Ä–æ–≤ –∑–¥–∞–Ω–∏–π"""
        contours, _ = cv2.findContours(
            binary_mask, 
            cv2.RETR_EXTERNAL, 
            cv2.CHAIN_APPROX_SIMPLE
        )
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–æ–Ω—Ç—É—Ä—ã (—à—É–º)
        min_area = 100  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å –≤ –ø–∏–∫—Å–µ–ª—è—Ö
        filtered_contours = [c for c in contours if cv2.contourArea(c) > min_area]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–ª–æ—â–∞–¥–∏ (–æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É)
        filtered_contours.sort(key=cv2.contourArea, reverse=True)
        
        return filtered_contours
    
    def _compute_features(self, mask: np.ndarray, contours: List[np.ndarray]) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∑–¥–∞–Ω–∏–π"""
        total_pixels = mask.shape[0] * mask.shape[1]
        building_pixels = np.sum(mask)
        building_ratio = building_pixels / total_pixels
        
        features = {
            'building_ratio': building_ratio,
            'num_buildings': len(contours),
            'total_area': building_pixels,
            'contour_areas': [cv2.contourArea(c) for c in contours]
        }
        
        if contours:
            # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –∑–¥–∞–Ω–∏—è
            largest_contour = contours[0]
            x, y, w, h = cv2.boundingRect(largest_contour)
            
            features['largest_building'] = {
                'area': cv2.contourArea(largest_contour),
                'bbox': (x, y, w, h),
                'aspect_ratio': w / h if h > 0 else 0,
                'perimeter': cv2.arcLength(largest_contour, True)
            }
        
        return features
    
    def compare_with_satellite(self, 
                               photo_segmentation: Dict[str, Any],
                               satellite_image_path: str) -> Optional[Dict[str, Any]]:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç—É—Ä–æ–≤ –∑–¥–∞–Ω–∏—è —Å —Ñ–æ—Ç–æ –∏ —Å–ø—É—Ç–Ω–∏–∫–æ–≤–æ–≥–æ —Å–Ω–∏–º–∫–∞
        
        Args:
            photo_segmentation: –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ñ–æ—Ç–æ
            satellite_image_path: –ü—É—Ç—å –∫ —Å–ø—É—Ç–Ω–∏–∫–æ–≤–æ–º—É —Å–Ω–∏–º–∫—É
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å similarity score
        """
        if not photo_segmentation or not os.path.exists(satellite_image_path):
            return None
        
        try:
            # –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º —Å–ø—É—Ç–Ω–∏–∫–æ–≤—ã–π —Å–Ω–∏–º–æ–∫
            satellite_segmentation = self.segment_buildings(satellite_image_path)
            
            if not satellite_segmentation:
                return None
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            similarity = self._compute_similarity(
                photo_segmentation['features'],
                satellite_segmentation['features']
            )
            
            result = {
                'success': True,
                'similarity': similarity,
                'photo_buildings': photo_segmentation['num_buildings'],
                'satellite_buildings': satellite_segmentation['num_buildings'],
                'match_confidence': similarity
            }
            
            logger.info(f"üó∫Ô∏è Satellite comparison: similarity={similarity:.2%}")
            
            return result
            
        except Exception as e:
            logger.error(f"Error comparing with satellite: {e}")
            return None
    
    def _compute_similarity(self, features1: Dict, features2: Dict) -> float:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –¥–≤—É–º—è –Ω–∞–±–æ—Ä–∞–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫:
        - –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–ª–æ—â–∞–¥–µ–π –∑–¥–∞–Ω–∏–π
        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–¥–∞–Ω–∏–π
        - –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –∑–¥–∞–Ω–∏—è
        """
        similarity_scores = []
        
        # 1. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø–ª–æ—â–∞–¥–µ–π
        ratio1 = features1.get('building_ratio', 0)
        ratio2 = features2.get('building_ratio', 0)
        if ratio1 > 0 and ratio2 > 0:
            ratio_similarity = 1 - abs(ratio1 - ratio2) / max(ratio1, ratio2)
            similarity_scores.append(ratio_similarity * 0.3)  # –≤–µ—Å 30%
        
        # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–¥–∞–Ω–∏–π
        num1 = features1.get('num_buildings', 0)
        num2 = features2.get('num_buildings', 0)
        if num1 > 0 and num2 > 0:
            num_similarity = 1 - abs(num1 - num2) / max(num1, num2)
            similarity_scores.append(num_similarity * 0.2)  # –≤–µ—Å 20%
        
        # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–∞–º–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ –∑–¥–∞–Ω–∏—è
        if 'largest_building' in features1 and 'largest_building' in features2:
            lb1 = features1['largest_building']
            lb2 = features2['largest_building']
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ aspect ratio
            ar1 = lb1.get('aspect_ratio', 0)
            ar2 = lb2.get('aspect_ratio', 0)
            if ar1 > 0 and ar2 > 0:
                ar_similarity = 1 - abs(ar1 - ar2) / max(ar1, ar2)
                similarity_scores.append(ar_similarity * 0.5)  # –≤–µ—Å 50%
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
        if similarity_scores:
            return sum(similarity_scores)
        else:
            return 0.0
    
    def visualize_segmentation(self, 
                              image_path: str, 
                              segmentation_result: Dict[str, Any],
                              output_path: Optional[str] = None) -> Optional[str]:
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        
        Args:
            image_path: –ü—É—Ç—å –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            segmentation_result: –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = cv2.imread(image_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # –°–æ–∑–¥–∞–µ–º overlay —Å –º–∞—Å–∫–æ–π
            mask = segmentation_result['mask']
            overlay = image.copy()
            overlay[mask > 0] = [0, 255, 0]  # –ó–µ–ª–µ–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è –∑–¥–∞–Ω–∏–π
            
            # –°–º–µ—à–∏–≤–∞–µ–º —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º
            result_image = cv2.addWeighted(image, 0.7, overlay, 0.3, 0)
            
            # –†–∏—Å—É–µ–º –∫–æ–Ω—Ç—É—Ä—ã
            contours = segmentation_result['contours']
            cv2.drawContours(result_image, contours, -1, (255, 0, 0), 2)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            if output_path is None:
                output_dir = Path("data/segmentation_results")
                output_dir.mkdir(parents=True, exist_ok=True)
                output_path = str(output_dir / f"seg_{Path(image_path).name}")
            
            result_image_bgr = cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR)
            cv2.imwrite(output_path, result_image_bgr)
            
            logger.info(f"üíæ Segmentation visualization saved to {output_path}")
            
            return output_path
            
        except Exception as e:
            logger.error(f"Error visualizing segmentation: {e}")
            return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–∏—Å–∞"""
        return {
            'model_loaded': self.model is not None,
            'device': self.device,
            'input_size': self.input_size,
            'model_type': 'DeepLabV3+ (ResNet50)'
        }
