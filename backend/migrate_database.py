#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –º–∏–≥—Ä–∞—Ü–∏–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö Geo Locator
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö –¥–æ –Ω–æ–≤–æ–π —Å—Ö–µ–º—ã.

–ó–∞–ø—É—Å–∫:
    python migrate_database.py

–§—É–Ω–∫—Ü–∏–∏:
- –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü
- –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
- –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü
- –ú–∏–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –≤–µ—Ä—Å–∏—è–º–∏
"""

import os
import sys
import logging
from datetime import datetime
from sqlalchemy import create_engine, text, inspect, MetaData, Table, Column, Integer, String, Float, Boolean, DateTime, Text, JSON
from sqlalchemy.exc import SQLAlchemyError

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
try:
    from dotenv import load_dotenv
    # –ò—â–µ–º .env —Ñ–∞–π–ª –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö
    env_path = None
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    if os.path.exists(os.path.join(current_dir, '.env')):
        env_path = os.path.join(current_dir, '.env')
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é (–∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞)
    elif os.path.exists(os.path.join(os.path.dirname(current_dir), '.env')):
        env_path = os.path.join(os.path.dirname(current_dir), '.env')
    
    if env_path:
        load_dotenv(env_path)
        print(f"‚úÖ Loaded environment variables from: {env_path}")
    else:
        print("‚ö†Ô∏è  .env file not found, using system environment variables")
        
except ImportError:
    print("‚ö†Ô∏è  python-dotenv not installed, using system environment variables")
    print("   Install with: pip install python-dotenv")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_database_url():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ URL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    database_url = os.getenv('DATABASE_URL')
    if database_url:
        return database_url
    
    host = os.getenv('POSTGRES_HOST', 'localhost')
    port = os.getenv('POSTGRES_PORT', '5432')
    database = os.getenv('POSTGRES_DB', 'geo_locator')
    username = os.getenv('POSTGRES_USER', 'postgres')
    password = os.getenv('POSTGRES_PASSWORD', 'password')
    
    return f"postgresql://{username}:{password}@{host}:{port}/{database}"

def check_table_exists(engine, table_name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã"""
    inspector = inspect(engine)
    return table_name in inspector.get_table_names()

def check_column_exists(engine, table_name, column_name):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ"""
    try:
        inspector = inspect(engine)
        columns = inspector.get_columns(table_name)
        return any(col['name'] == column_name for col in columns)
    except:
        return False

def add_missing_columns(engine):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã"""
    migrations = [
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü—É photos –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        {
            'table': 'photos',
            'columns': [
                ('original_filename', 'VARCHAR(255)'),
                ('file_size', 'INTEGER'),
                ('mime_type', 'VARCHAR(100)'),
                ('width', 'INTEGER'),
                ('height', 'INTEGER'),
                ('camera_make', 'VARCHAR(100)'),
                ('camera_model', 'VARCHAR(100)'),
                ('date_taken', 'TIMESTAMP'),
                ('processed', 'BOOLEAN DEFAULT FALSE'),
            ]
        },
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü—É violations –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        {
            'table': 'violations',
            'columns': [
                ('source', 'VARCHAR(50)'),
                ('bbox_data', 'JSON'),
                ('status', 'VARCHAR(50) DEFAULT \'detected\''),
                ('priority', 'VARCHAR(20) DEFAULT \'medium\''),
                ('updated_at', 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'),
            ]
        },
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü—É users –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        {
            'table': 'users',
            'columns': [
                ('full_name', 'VARCHAR(200)'),
                ('role', 'VARCHAR(50) DEFAULT \'user\''),
                ('is_active', 'BOOLEAN DEFAULT TRUE'),
                ('created_at', 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP'),
                ('last_login', 'TIMESTAMP'),
            ]
        }
    ]
    
    for migration in migrations:
        table_name = migration['table']
        
        if not check_table_exists(engine, table_name):
            logger.info(f"‚è≠Ô∏è  Table {table_name} doesn't exist, skipping column migration")
            continue
            
        logger.info(f"üîç Checking table {table_name} for missing columns...")
        
        for column_name, column_def in migration['columns']:
            if not check_column_exists(engine, table_name, column_name):
                try:
                    sql = f"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_def};"
                    with engine.connect() as conn:
                        conn.execute(text(sql))
                        conn.commit()
                    logger.info(f"‚úÖ Added column {column_name} to {table_name}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Could not add column {column_name} to {table_name}: {e}")

def create_missing_tables(engine):
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º SQL –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü
    table_definitions = {
        'geo_images': '''
            CREATE TABLE geo_images (
                id SERIAL PRIMARY KEY,
                filename VARCHAR(255) NOT NULL,
                file_path VARCHAR(500) NOT NULL,
                file_hash VARCHAR(64) UNIQUE NOT NULL,
                file_size INTEGER,
                latitude FLOAT,
                longitude FLOAT,
                altitude FLOAT,
                accuracy FLOAT,
                width INTEGER,
                height INTEGER,
                format VARCHAR(10),
                camera_make VARCHAR(100),
                camera_model VARCHAR(100),
                date_taken TIMESTAMP,
                date_uploaded TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                date_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                address TEXT,
                city VARCHAR(100),
                region VARCHAR(100),
                country VARCHAR(100),
                tags TEXT,
                description TEXT,
                user_notes TEXT,
                processed BOOLEAN DEFAULT FALSE,
                has_gps BOOLEAN DEFAULT FALSE,
                geo_source VARCHAR(50),
                yandex_place_id VARCHAR(100),
                dgis_place_id VARCHAR(100),
                satellite_match_score FLOAT
            );
        ''',
        
        'detected_objects': '''
            CREATE TABLE detected_objects (
                id SERIAL PRIMARY KEY,
                class_name VARCHAR(100) NOT NULL,
                confidence FLOAT NOT NULL,
                source VARCHAR(50) DEFAULT 'yolo',
                bbox_x FLOAT,
                bbox_y FLOAT,
                bbox_width FLOAT,
                bbox_height FLOAT,
                bbox_normalized BOOLEAN DEFAULT TRUE,
                violation_id INTEGER REFERENCES violations(id),
                photo_id INTEGER REFERENCES photos(id) NOT NULL,
                detected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        ''',
        
        'analysis_sessions': '''
            CREATE TABLE analysis_sessions (
                id SERIAL PRIMARY KEY,
                session_id VARCHAR(100) UNIQUE NOT NULL,
                user_id INTEGER REFERENCES users(id) NOT NULL,
                session_type VARCHAR(50),
                total_files INTEGER DEFAULT 0,
                processed_files INTEGER DEFAULT 0,
                successful_files INTEGER DEFAULT 0,
                failed_files INTEGER DEFAULT 0,
                status VARCHAR(50) DEFAULT 'running',
                started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                completed_at TIMESTAMP,
                results_summary JSON,
                error_log TEXT
            );
        ''',
        
        'system_logs': '''
            CREATE TABLE system_logs (
                id SERIAL PRIMARY KEY,
                level VARCHAR(20) NOT NULL,
                message TEXT NOT NULL,
                module VARCHAR(100),
                user_id INTEGER REFERENCES users(id),
                session_id VARCHAR(100),
                request_id VARCHAR(100),
                extra_data JSON,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );
        '''
    }
    
    for table_name, table_sql in table_definitions.items():
        if not check_table_exists(engine, table_name):
            try:
                with engine.connect() as conn:
                    conn.execute(text(table_sql))
                    conn.commit()
                logger.info(f"‚úÖ Created table {table_name}")
            except Exception as e:
                logger.error(f"‚ùå Error creating table {table_name}: {e}")
        else:
            logger.info(f"‚úÖ Table {table_name} already exists")

def update_data_types(engine):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    
    updates = [
        # –û–±–Ω–æ–≤–ª—è–µ–º address_data –≤ photos –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ JSON
        {
            'condition': lambda: check_table_exists(engine, 'photos') and not check_column_exists(engine, 'photos', 'address_data'),
            'sql': 'ALTER TABLE photos ADD COLUMN address_data JSON;',
            'description': 'Add address_data JSON column to photos'
        },
        
        # –û–±–Ω–æ–≤–ª—è–µ–º bbox_data –≤ violations –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ JSON
        {
            'condition': lambda: check_table_exists(engine, 'violations') and not check_column_exists(engine, 'violations', 'bbox_data'),
            'sql': 'ALTER TABLE violations ADD COLUMN bbox_data JSON;',
            'description': 'Add bbox_data JSON column to violations'
        }
    ]
    
    for update in updates:
        if update['condition']():
            try:
                with engine.connect() as conn:
                    conn.execute(text(update['sql']))
                    conn.commit()
                logger.info(f"‚úÖ {update['description']}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  {update['description']} failed: {e}")

def create_views(engine):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–µ–∑–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π (views)"""
    
    views = [
        {
            'name': 'violation_summary',
            'sql': '''
                CREATE OR REPLACE VIEW violation_summary AS
                SELECT 
                    v.category,
                    v.status,
                    v.priority,
                    COUNT(*) as count,
                    AVG(v.confidence) as avg_confidence,
                    MIN(v.detected_at) as first_detected,
                    MAX(v.detected_at) as last_detected
                FROM violations v
                GROUP BY v.category, v.status, v.priority
                ORDER BY count DESC;
            '''
        },
        
        {
            'name': 'user_activity',
            'sql': '''
                CREATE OR REPLACE VIEW user_activity AS
                SELECT 
                    u.username,
                    u.full_name,
                    COUNT(DISTINCT p.id) as photos_uploaded,
                    COUNT(DISTINCT v.id) as violations_detected,
                    MAX(p.uploaded_at) as last_upload,
                    u.last_login
                FROM users u
                LEFT JOIN photos p ON u.id = p.user_id
                LEFT JOIN violations v ON u.id = v.user_id
                GROUP BY u.id, u.username, u.full_name, u.last_login
                ORDER BY photos_uploaded DESC;
            '''
        },
        
        {
            'name': 'coordinate_coverage',
            'sql': '''
                CREATE OR REPLACE VIEW coordinate_coverage AS
                SELECT 
                    'photos' as source_table,
                    COUNT(*) as total_records,
                    COUNT(CASE WHEN lat IS NOT NULL AND lon IS NOT NULL THEN 1 END) as with_coordinates,
                    ROUND(
                        COUNT(CASE WHEN lat IS NOT NULL AND lon IS NOT NULL THEN 1 END) * 100.0 / COUNT(*), 
                        2
                    ) as coverage_percentage
                FROM photos
                UNION ALL
                SELECT 
                    'violations' as source_table,
                    COUNT(*) as total_records,
                    COUNT(CASE WHEN lat IS NOT NULL AND lon IS NOT NULL THEN 1 END) as with_coordinates,
                    ROUND(
                        COUNT(CASE WHEN lat IS NOT NULL AND lon IS NOT NULL THEN 1 END) * 100.0 / COUNT(*), 
                        2
                    ) as coverage_percentage
                FROM violations;
            '''
        }
    ]
    
    for view in views:
        try:
            with engine.connect() as conn:
                conn.execute(text(view['sql']))
                conn.commit()
            logger.info(f"‚úÖ Created/updated view {view['name']}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not create view {view['name']}: {e}")

def analyze_database(engine):
    """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    
    logger.info("üìä Analyzing database structure...")
    
    inspector = inspect(engine)
    tables = inspector.get_table_names()
    
    logger.info(f"üìã Found {len(tables)} tables:")
    
    for table in sorted(tables):
        try:
            with engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
                count = result.scalar()
            
            columns = inspector.get_columns(table)
            logger.info(f"   üìÑ {table}: {count} records, {len(columns)} columns")
            
        except Exception as e:
            logger.warning(f"   ‚ö†Ô∏è  Could not analyze table {table}: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏"""
    logger.info("üîÑ Starting Geo Locator database migration...")
    
    database_url = get_database_url()
    logger.info(f"Database URL: {database_url.replace(database_url.split('@')[0].split('//')[1], '***')}")
    
    try:
        engine = create_engine(database_url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        logger.info("‚úÖ Database connection successful")
        
        # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        analyze_database(engine)
        
        # 2. –°–æ–∑–¥–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        logger.info("üèóÔ∏è  Creating missing tables...")
        create_missing_tables(engine)
        
        # 3. –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        logger.info("üîß Adding missing columns...")
        add_missing_columns(engine)
        
        # 4. –û–±–Ω–æ–≤–ª—è–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        logger.info("üîÑ Updating data types...")
        update_data_types(engine)
        
        # 5. –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        logger.info("üëÅÔ∏è  Creating database views...")
        create_views(engine)
        
        # 6. –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        logger.info("üìä Final database analysis:")
        analyze_database(engine)
        
        logger.info("üéâ Database migration completed successfully!")
        
    except Exception as e:
        logger.error(f"‚ùå Migration failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
